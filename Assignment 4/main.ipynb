{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5dc91d5-c618-4f0e-a31a-0b0271cf074a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 4 ... 1 3 1]\n",
      "1024\n",
      "Elapsed time using GPU (sec):  0.001490880012512207\n",
      "---------------------\n",
      "Original array: [1 2 4 ... 1 3 1]\n",
      "Array after left rotation: [2 4 1 ... 3 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Task 1\n",
    "\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "import numpy as np\n",
    "from pycuda.compiler import SourceModule\n",
    "import time \n",
    "\n",
    "#################### Satrt GPU timing\n",
    "start_gpu = cuda.Event()\n",
    "end_gpu = cuda.Event()\n",
    "start_gpu.record()\n",
    "\n",
    "# CUDA kernel for left rotation\n",
    "kernel_code = \"\"\"\n",
    "__global__ void left_rotation(int *in, int *out, int size) {\n",
    "    // int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    // Since we assume that there's only one block with 1024 threads we can just use the following\n",
    "    int idx = threadIdx.x;\n",
    "    if (idx < size) {\n",
    "        int shifted_idx = (idx - 1 + size) % size;\n",
    "        out[shifted_idx] = in[idx];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Compile the CUDA kernel\n",
    "mod = SourceModule(kernel_code)\n",
    "left_rotation = mod.get_function(\"left_rotation\")\n",
    "\n",
    "# Initialize input array with size=1024 since it's the highest and allows us to see the more differences\n",
    "input_size = 1024\n",
    "host_array = np.random.randint(low=0, high=10, size=input_size, dtype=np.int32)\n",
    "print(host_array)\n",
    "print(input_size)\n",
    "\n",
    "# Allocate memory on GPU\n",
    "device_array = cuda.mem_alloc(host_array.nbytes)\n",
    "device_output = cuda.mem_alloc(host_array.nbytes)\n",
    "\n",
    "# Copy data to GPU\n",
    "cuda.memcpy_htod(device_array, host_array)\n",
    "\n",
    "# Define block and grid size\n",
    "# Since we assume only one block with 1024 threads\n",
    "block_size = input_size\n",
    "grid_size = 1\n",
    "\n",
    "\n",
    "# Launch kernel\n",
    "left_rotation(device_array, device_output, np.int32(input_size), block=(block_size, 1, 1), grid=(grid_size, 1))\n",
    "\n",
    "# Copy result back to CPU\n",
    "host_output = np.empty_like(host_array)\n",
    "cuda.memcpy_dtoh(host_output, device_output)\n",
    "\n",
    "#################### End GPU timing\n",
    "end_gpu.record()\n",
    "cuda.Context.synchronize()\n",
    "gpu_time = start_gpu.time_till(end_gpu)*1e-3\n",
    "print(\"Elapsed time using GPU (sec): \", gpu_time)\n",
    "print(\"---------------------\")\n",
    "\n",
    "# Print result\n",
    "print(\"Original array:\", host_array)\n",
    "print(\"Array after left rotation:\", host_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f89ff335-f8b0-4d9b-b2f2-43e578f33c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time using sequential for-loop (sec):  0.00049591064453125\n",
      "---------------------\n",
      "[2. 4. 1. ... 3. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# CPU Sequential implementation (naive)\n",
    "\n",
    "#################### Start CPU timing\n",
    "start_cpu = time.time()\n",
    "\n",
    "naive_out = np.empty(input_size)\n",
    "for idx, item in enumerate(host_array):\n",
    "    naive_out[(idx-1)%input_size] = item\n",
    "    \n",
    "#################### End CPU timing\n",
    "end_cpu = time.time()\n",
    "cpu_time = end_cpu - start_cpu\n",
    "print(\"Elapsed time using sequential for-loop (sec): \", cpu_time)\n",
    "print(\"---------------------\")\n",
    "    \n",
    "print(naive_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fecad58d-f8b5-46ea-b5c4-ae44382ec470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time using pythonic way (sec):  8.153915405273438e-05\n",
      "---------------------\n",
      "[2 4 1 ... 3 1 1]\n"
     ]
    }
   ],
   "source": [
    "# CPU sequential implementation (pythonic)\n",
    "\n",
    "#################### Start CPU timing\n",
    "start_cpu = time.time()\n",
    "\n",
    "pythonic_out = np.concatenate((host_array[1:], host_array[:1])) \n",
    "\n",
    "#################### End CPU timing\n",
    "end_cpu = time.time()\n",
    "cpu_time = end_cpu - start_cpu\n",
    "print(\"Elapsed time using pythonic way (sec): \", cpu_time)\n",
    "print(\"---------------------\")\n",
    "\n",
    "print(pythonic_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "417db96d-4073-4872-afff-16ac673e8def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output OK\n"
     ]
    }
   ],
   "source": [
    "# Task 2\n",
    "# Compare the different implementations\n",
    "if (host_output == pythonic_out).all() and (pythonic_out == np.array(naive_out, dtype=np.int32)).all():\n",
    "    print(\"Output OK\")\n",
    "else:\n",
    "    print(\"Output do not match, please investigate!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6a39d-adc7-4548-9442-4d40c38a36db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
